# -*- coding: utf-8 -*-
"""fcc_sms_text_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/freeCodeCamp/boilerplate-neural-network-sms-text-classifier/blob/master/fcc_sms_text_classification.ipynb
"""

# import libraries
try:
  # %tensorflow_version only exists in Colab.
  !pip install tf-nightly
except Exception:
  pass
import tensorflow as tf
import pandas as pd
from tensorflow import keras
!pip install tensorflow-datasets
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)

# get data files
!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv
!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv

train_file_path = "train-data.tsv"
test_file_path = "valid-data.tsv"

train_data = pd.read_csv(train_file_path, sep='\t', header=None, names=['label', 'text'])
test_data = pd.read_csv(test_file_path, sep='\t', header=None, names=['label', 'text'])

# Convert labels to numerical (0 for 'ham', 1 for 'spam')
train_data['label'] = train_data['label'].map({'ham': 0, 'spam': 1})
test_data['label'] = test_data['label'].map({'ham': 0, 'spam': 1})

# Tokenize the text data
tokenizer = keras.preprocessing.text.Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(train_data['text'])

# Convert text to sequences
train_sequences = tokenizer.texts_to_sequences(train_data['text'])
test_sequences = tokenizer.texts_to_sequences(test_data['text'])

# Pad sequences to ensure equal length
max_length = 100
train_padded = keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=max_length, padding='post')
test_padded = keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=max_length, padding='post')

# Extract labels
train_labels = np.array(train_data['label'])
test_labels = np.array(test_data['label'])

#define the model
# Define the model
model = keras.Sequential([
    keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=max_length),
    keras.layers.GlobalAveragePooling1D(),
    keras.layers.Dense(16, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
])

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(train_padded, train_labels, epochs=10, validation_data=(test_padded, test_labels))

# function to predict messages based on model
# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])
# function to predict messages based on model
def predict_message(pred_text):
    # Preprocess the input text
    sequence = tokenizer.texts_to_sequences([pred_text])
    padded_sequence = keras.preprocessing.sequence.pad_sequences(sequence, maxlen=max_length, padding='post')

    # Make a prediction
    prediction = model.predict(padded_sequence)[0][0]

    # Return the result as [prediction_score, label]
    if prediction > 0.5:
        return [prediction, 'spam']
    else:
        return [prediction, 'ham']

# Example prediction
pred_text = "how are you doing today?"
prediction = predict_message(pred_text)
print(prediction)

def predict_message(pred_text):
    # Tokenize and pad the message
    sequence = tokenizer.texts_to_sequences([pred_text])
    padded_sequence = keras.preprocessing.sequence.pad_sequences(sequence, maxlen=max_length, padding='post')

    # Get the model's prediction (between 0 and 1)
    prediction = model.predict(padded_sequence)[0][0]

    # Adjust threshold and return prediction and label
    if prediction > 0.5:
        return [prediction, 'spam']
    else:
        return [prediction, 'ham']

# Example message for testing
pred_text = "how are you doing today?"
prediction = predict_message(pred_text)
print(prediction)